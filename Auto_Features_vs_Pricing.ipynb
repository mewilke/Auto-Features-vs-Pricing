{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GOjUQVf2DFr"
      },
      "outputs": [],
      "source": [
        "# What features of used cars increase the prices of the cars?\n",
        "# The assumption is that the sale of a higher priced car nets more money for a used car dealer. There is the risk that this may not be true.\n",
        "# Based on a Kaggle dataset of used cars, determine which features are most correlated with car prices. www.kaggle.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7jQM-ZO2DFr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "from scipy.linalg import svd\n",
        "from scipy.stats import zscore\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1rQpDS-2DFs"
      },
      "outputs": [],
      "source": [
        "cars = pd.read_csv('data/vehicles.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5O_L32Um2DFs"
      },
      "outputs": [],
      "source": [
        "cars.info()\n",
        "print(cars.head())\n",
        "print(cars.tail())\n",
        "cars.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsanK5k92DFs"
      },
      "outputs": [],
      "source": [
        "cars.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2iK_t8Gcxze"
      },
      "outputs": [],
      "source": [
        "# The VIN contains a lot of information about a car not included in the dataset.\n",
        "# The VIN numbers varied depending in the manufacturer before 1981.\n",
        "# Dropping rows before 1981 to allow use of VIN and\n",
        "# because used car dealers are not likely to see many cars that old.\n",
        "# Also assuming there is not a big market for antique cars although they can bring very high prices.\n",
        "rows_to_drop = cars[cars['year'] < 1981].index\n",
        "cars_cleaned = cars.drop(rows_to_drop)\n",
        "\n",
        "# split VIN into it's components and keep columns not already included in data\n",
        "cars_cleaned['assembly_country'] = cars_cleaned['VIN'].str[:1] # final assembly country\n",
        "cars_cleaned['type_or_division'] = cars_cleaned['VIN'].str[2:3] # type or manufacturing division\n",
        "cars_cleaned['attributes'] = cars_cleaned['VIN'].str[3:7] # attributes, such as model, body style, engine type, transmission type etc.\n",
        "cars_cleaned['assembly_plant'] = cars_cleaned['VIN'].str[10:11] # assembly plant\n",
        "vehicles = cars_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfGt5wIaf6l6"
      },
      "outputs": [],
      "source": [
        "# Convert text to numbers to allow correlation calculations\n",
        "\n",
        "# not working with these datatypes in the interest of time.\n",
        "# dropping these because one-hot encoding will increase features a lot:\n",
        "# region, model, state. I do think these are relevant features.\n",
        "\n",
        "# Ignore Future Warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "vehicles.drop(columns=(['region', 'model', 'state']), axis=1, inplace=True)\n",
        "\n",
        "# Assigning numeric values to allow correlation. This may introduce some bias\n",
        "# because it requires making assumptions about what values are better than others.\n",
        "# However, it prevents adding many features if one-hot encoding were used.\n",
        "vehicles['condition#'] = vehicles['condition'].replace({'new':5, 'excellent':4, 'like new':3, 'good':2, 'fair':1, 'salvage':0})\n",
        "vehicles['cylinders#'] = vehicles['cylinders'].replace({'other':0, '3 cylinders':3, '4 cylinders':4, '5 cylinders':5, '6 cylinders':6, '8 cylinders':8, '10 cylinders':10, '12 cylinders':12})\n",
        "vehicles['title_status#'] = vehicles['title_status'].replace({'clean':5, 'lien':4, 'rebuilt':3, 'missing':2, 'salvage':0, 'parts only':1})\n",
        "vehicles['size#'] = vehicles['size'].replace({'full-size':3, 'mid-size':2, 'compact':1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11_3dQCi2DFs"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfO8U6xxZoA4"
      },
      "outputs": [],
      "source": [
        "# drop duplicates\n",
        "vehicles.drop_duplicates(inplace=True)\n",
        "# drop rows with NAN\n",
        "vehicles.dropna(inplace=True)\n",
        "# making assumption that all cars have some value even as scrap or parts\n",
        "rows_to_drop = vehicles[vehicles['price'] == 0].index\n",
        "vehicles = vehicles.drop(rows_to_drop)\n",
        "vehicles['year'] = vehicles['year'].astype(int) # year is an int not a float\n",
        "# drop erroneous price row\n",
        "rows_to_drop = vehicles[vehicles['price'] == 12345678].index\n",
        "vehicles.drop(rows_to_drop, inplace=True)\n",
        "vehicles.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXV_RGwX2DFs"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding\n",
        "# One-hot encode fuel because unsure what the order is and only adds 3 features\n",
        "cars_cleaned2 = pd.get_dummies(vehicles, columns=['fuel'])\n",
        "# One-hot encode transmission because unsure what order is and only adds 2 features\n",
        "cars_cleaned3 = pd.get_dummies(cars_cleaned2, columns=['transmission'])\n",
        "# One-hot encode drive because unsure what order is and only adds 2 features\n",
        "cars_cleaned4 = pd.get_dummies(cars_cleaned3, columns=['drive'])\n",
        "# One-hot encode cylinders because 0 cylinders do not plot well with other cylinder values\n",
        "cars_cleaned5 = pd.get_dummies(cars_cleaned4, columns=['cylinders#'])\n",
        "\n",
        "# drop the id feature assuming it has no correlation with price\n",
        "cars_cleaned5.drop('id', axis=1, inplace=True)\n",
        "vehicles = cars_cleaned5\n",
        "\n",
        "# Set one-hot endoding from True/False to 1/0\n",
        "vehicles.replace(({True:1, False:0}), inplace=True)\n",
        "vehicles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOs589dN2DFs"
      },
      "outputs": [],
      "source": [
        "vehicles.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ_5kbWHkbLy"
      },
      "outputs": [],
      "source": [
        "# remove outliers with Z-score >= 3\n",
        "vehicles_numeric = vehicles.select_dtypes(include=[np.number])\n",
        "for i in vehicles_numeric.columns:\n",
        "  vehicles_numeric['Z_score'] = zscore(vehicles_numeric[i])\n",
        "  vehicles_cleaned = vehicles_numeric[np.abs(vehicles_numeric['Z_score']) <= 3].copy()\n",
        "vehicles_cleaned.drop('Z_score', axis=1, inplace=True)\n",
        "vehicles_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci3DIDnQ2DFs"
      },
      "outputs": [],
      "source": [
        "vehicles_cleaned.corr()\n",
        "# all columns have some correlation with price.\n",
        "vehicles_cleaned.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6f6X8NrvYcyv"
      },
      "outputs": [],
      "source": [
        "price_corr = (vehicles_cleaned.corr()['price']).abs()\n",
        "price_corr.sort_values(ascending=False)\n",
        "# Note that using the number of cylinders as the value correlates better\n",
        "# with price than if numbers assigned based on guess at most popular\n",
        "# number of cylinders (e.g. 6 cylinders preferred over 4).\n",
        "# It is not surprising that year and odometer have a correlation of about 0.5.\n",
        "# If the correlation were higher, I might eliminate the odometer as a feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbNEGzLV2DFs"
      },
      "outputs": [],
      "source": [
        "# feature selection\n",
        "def svd_norm(X): #normalizes data & runs PCA\n",
        "  x_norm =( X - X.mean())/X.std()\n",
        "  U, sigma, VT = svd(x_norm, full_matrices=False)\n",
        "  Sigma = np.diag(sigma)\n",
        "  return U, Sigma, VT\n",
        "\n",
        "U, Sigma, VT = svd_norm(vehicles_numeric)\n",
        "sig = np.diag(Sigma)\n",
        "sigsum = sig.cumsum()/sig.sum()\n",
        "plt.plot(sigsum, 'go-')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "plt.plot(sig, 'ro-')\n",
        "# r = 6 features with excellent correlation to price. Could add 3 more features.\n",
        "r = 6\n",
        "# Can run with the most important features or with all the features\n",
        "#car_short = vehicles_numeric[['price', 'year', 'fuel_diesel', 'fuel_gas', 'drive_fwd', 'cylinders#_8.0', 'drive_4wd']]\n",
        "car_short = vehicles_numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA2_LmJZEet6"
      },
      "outputs": [],
      "source": [
        "def pca(X, r): #projects data\n",
        "    x_norm =( X - X.mean())/X.std()\n",
        "    U, sigma, VT = svd(x_norm, full_matrices=False)\n",
        "    Sigma = np.diag(sigma)\n",
        "    Ur = U[:, :r]\n",
        "    Sigma_r = Sigma[:r, :r]\n",
        "    return pd.DataFrame(Ur @ Sigma_r, columns=[f'pca_{i}' for i in range(1, r + 1)])\n",
        "pca(vehicles_numeric, r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqxh_-tE2DFs"
      },
      "source": [
        "### Modeling\n",
        "\n",
        "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiQIWere2DFs"
      },
      "outputs": [],
      "source": [
        "car_short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyiP2yZyyjQh"
      },
      "outputs": [],
      "source": [
        "# Split the target from the features\n",
        "X = car_short.drop('price', axis=1)\n",
        "y = car_short['price']\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r83PQcgx3q05"
      },
      "outputs": [],
      "source": [
        "# fit linear\n",
        "X_train_feat = pd.DataFrame(X_train['year'])\n",
        "X_test_feat = pd.DataFrame(X_test['year'])\n",
        "lr = LinearRegression(fit_intercept=True)\n",
        "lr.fit(X_train_feat, y_train)\n",
        "\n",
        "# year appears curved relative to price. Degree 3 gives lower MSE than degree 2.\n",
        "# However, if fit_intercept=True, linear produces the lowest MSE.\n",
        "# Making the assumption there are few buyers for classic cars although the prices are higher.\n",
        "# Comfortable underestimating the prices of older cars, including negative values, in giving advice\n",
        "# to used car dealers because they will make fewer sales and therefore less money on classic cars.\n",
        "# Classic cars are a different market.\n",
        "# Proceeding with the linear model and fit_intercept=True.\n",
        "poly_transform = PolynomialFeatures(degree=3, include_bias=False)\n",
        "X_poly_year = poly_transform.fit_transform(X_train_feat)\n",
        "lr_3 = LinearRegression(fit_intercept=True)\n",
        "lr_3.fit(X_poly_year, y_train)\n",
        "y_pred_poly = lr_3.predict(X_poly_year)\n",
        "\n",
        "# plot predictions over real data\n",
        "X_grid = np.arange(min(X_train_feat.values), max(X_train_feat.values), 1)\n",
        "X_grid = X_grid.reshape((len(X_grid), 1))\n",
        "plt.scatter(X_train_feat, y_train)\n",
        "plt.scatter(X_train_feat, y_pred_poly, color='red')\n",
        "plt.plot(X_train_feat, lr.predict(X_train_feat), color='green')\n",
        "plt.title('Linear and Polynomial (degree=3)')\n",
        "plt.xlabel('Manufacturing Year')\n",
        "plt.ylabel('Price ($)')\n",
        "plt.legend(['Real Data', 'Polynomial (3)', 'Linear Prediction'])\n",
        "\n",
        "mse_lin = mean_squared_error(X_test['year'], lr.predict(X_test_feat))\n",
        "mae_lin = mean_absolute_error(X_test['year'], lr.predict(X_test_feat))\n",
        "rmse_lin = np.sqrt(mse_lin)\n",
        "r2_lin = r2_score(X_test['year'], lr.predict(X_test_feat))\n",
        "print('Linear statistics')\n",
        "print(f'RMSE: {rmse_lin}')\n",
        "print(f'r2: {r2_lin}')\n",
        "print(f'MSE: {mse_lin}')\n",
        "print(f'MAE: {mae_lin}\\n')\n",
        "\n",
        "mse_poly = mean_squared_error(X_test['year'], lr_3.predict(poly_transform.fit_transform(X_test_feat)))\n",
        "mae_poly = mean_absolute_error(X_test['year'], lr_3.predict(poly_transform.fit_transform(X_test_feat)))\n",
        "rmse_poly = np.sqrt(mse_poly)\n",
        "r2_poly = r2_score(X_test['year'], lr_3.predict(poly_transform.fit_transform(X_test_feat)))\n",
        "print('Polynomial 3 statistics')\n",
        "print(f'RMSE: {rmse_poly}')\n",
        "print(f'r2: {r2_poly}')\n",
        "print(f'MSE: {mse_poly}')\n",
        "print(f'MAE: {mae_poly}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4PKDF5-H7k9"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_-crZjdL0L9"
      },
      "outputs": [],
      "source": [
        "# fit linear\n",
        "X_train_feat = pd.DataFrame(X_train['fuel_diesel'])\n",
        "X_test_feat = pd.DataFrame(X_test['fuel_diesel'])\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "lr.fit(X_train_feat, y_train)\n",
        "\n",
        "# plot prediction over real data\n",
        "plt.scatter(X_train_feat, y_train)\n",
        "plt.plot(X_train_feat, lr.predict(X_train_feat), color='green')\n",
        "plt.title('Linear Regression')\n",
        "plt.xlabel('Diesel Fuel')\n",
        "plt.ylabel('Price')\n",
        "plt.legend(['Real Data', 'Linear Prediction'])\n",
        "\n",
        "# Calculate statistics\n",
        "mse = mean_squared_error(X_test_feat, lr.predict(X_test_feat))\n",
        "mae = mean_absolute_error(X_test_feat, lr.predict(X_test_feat))\n",
        "rmse = np.sqrt(mse_lin)\n",
        "r2 = r2_score(X_test_feat, lr.predict(X_test_feat))\n",
        "\n",
        "# Print statistics\n",
        "print(f'RMSE: {mse}')\n",
        "print(f'r2: {r2}')\n",
        "print(f'MSE: {mse}')\n",
        "print(f'MAE: {mae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq9Jv2vw0KTP"
      },
      "outputs": [],
      "source": [
        "# fit linear\n",
        "X_train_feat = pd.DataFrame(X_train['fuel_gas'])\n",
        "X_test_feat = pd.DataFrame(X_test['fuel_gas'])\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "lr.fit(X_train_feat, y_train)\n",
        "\n",
        "# plot prediction over real data\n",
        "#plt.scatter(X_train_feat, y_train)\n",
        "#plt.plot(X_train_feat, lr.predict(X_train_feat), color='green')\n",
        "#plt.title('Linear Regression')\n",
        "#plt.xlabel('Gas Fuel')\n",
        "#plt.ylabel('Price')\n",
        "#plt.legend(['Real Data', 'Linear Prediction'])\n",
        "\n",
        "# Calculate statistics\n",
        "mse = mean_squared_error(X_test_feat, lr.predict(X_test_feat))\n",
        "mae = mean_absolute_error(X_test_feat, lr.predict(X_test_feat))\n",
        "rmse = np.sqrt(mse_lin)\n",
        "r2 = r2_score(X_test_feat, lr.predict(X_test_feat))\n",
        "\n",
        "# Print statistics\n",
        "print(f'RMSE: {mse}')\n",
        "print(f'r2: {r2}')\n",
        "print(f'MSE: {mse}')\n",
        "print(f'MAE: {mae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9jBc3Dd1pHv"
      },
      "outputs": [],
      "source": [
        "# fit linear\n",
        "X_train_feat = pd.DataFrame(X_train['drive_fwd'])\n",
        "X_test_feat = pd.DataFrame(X_test['drive_fwd'])\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "lr.fit(X_train_feat, y_train)\n",
        "\n",
        "# plot prediction over real data\n",
        "#plt.scatter(X_train_feat, y_train)\n",
        "#plt.plot(X_train_feat, lr.predict(X_train_feat), color='green')\n",
        "#plt.title('Linear Regression')\n",
        "#plt.xlabel('Front Wheel Drive')\n",
        "#plt.ylabel('Price')\n",
        "#plt.legend(['Real Data', 'Linear Prediction'])\n",
        "\n",
        "# Calculate statistics\n",
        "mse = mean_squared_error(X_test_feat, lr.predict(X_test_feat))\n",
        "mae = mean_absolute_error(X_test_feat, lr.predict(X_test_feat))\n",
        "rmse = np.sqrt(mse_lin)\n",
        "r2 = r2_score(X_test_feat, lr.predict(X_test_feat))\n",
        "\n",
        "# Print statistics\n",
        "print(f'RMSE: {mse}')\n",
        "print(f'r2: {r2}')\n",
        "print(f'MSE: {mse}')\n",
        "print(f'MAE: {mae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l276NCUI18MK"
      },
      "outputs": [],
      "source": [
        "# fit linear\n",
        "X_train_feat = pd.DataFrame(X_train['cylinders#_8.0'])\n",
        "X_test_feat = pd.DataFrame(X_test['cylinders#_8.0'])\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "lr.fit(X_train_feat, y_train)\n",
        "\n",
        "# plot prediction over real data\n",
        "#plt.scatter(X_train_feat, y_train)\n",
        "#plt.plot(X_train_feat, lr.predict(X_train_feat), color='green')\n",
        "#plt.title('Linear Regression')\n",
        "#plt.xlabel('8 Cylinders')\n",
        "#plt.ylabel('Price')\n",
        "#plt.legend(['Real Data', 'Linear Prediction'])\n",
        "\n",
        "# Calculate statistics\n",
        "mse = mean_squared_error(X_test_feat, lr.predict(X_test_feat))\n",
        "mae = mean_absolute_error(X_test_feat, lr.predict(X_test_feat))\n",
        "rmse = np.sqrt(mse_lin)\n",
        "r2 = r2_score(X_test_feat, lr.predict(X_test_feat))\n",
        "\n",
        "# Print statistics\n",
        "print(f'RMSE: {mse}')\n",
        "print(f'r2: {r2}')\n",
        "print(f'MSE: {mse}')\n",
        "print(f'MAE: {mae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQbEykH59fF2"
      },
      "outputs": [],
      "source": [
        "# fit linear\n",
        "X_train_feat = pd.DataFrame(X_train['drive_4wd'])\n",
        "X_test_feat = pd.DataFrame(X_test['drive_4wd'])\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "lr.fit(X_train_feat, y_train)\n",
        "\n",
        "# plot prediction over real data\n",
        "#plt.scatter(X_train_feat, y_train)\n",
        "#plt.plot(X_train_feat, lr.predict(X_train_feat), color='green')\n",
        "#plt.title('Linear Regression')\n",
        "#plt.xlabel('4-Wheel Drive')\n",
        "#plt.ylabel('Price')\n",
        "#plt.legend(['Real Data', 'Linear Prediction'])\n",
        "\n",
        "# Calculate statistics\n",
        "mse = mean_squared_error(X_test_feat, lr.predict(X_test_feat))\n",
        "mae = mean_absolute_error(X_test_feat, lr.predict(X_test_feat))\n",
        "rmse = np.sqrt(mse_lin)\n",
        "r2 = r2_score(X_test_feat, lr.predict(X_test_feat))\n",
        "\n",
        "# Plot statistics\n",
        "print(f'RMSE: {mse}')\n",
        "print(f'r2: {r2}')\n",
        "print(f'MSE: {mse}')\n",
        "print(f'MAE: {mae}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW4rZXWqpmf2"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train, dev, and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.1, random_state=42)\n",
        "\n",
        "# Scale and fit the data\n",
        "pipe = Pipeline([\n",
        "  ('scaler', StandardScaler()),\n",
        "  ('ridge', Ridge())\n",
        "])\n",
        "scaled_pipe = pipe.fit(X_train, y_train)\n",
        "train_preds = scaled_pipe.predict(X_train)\n",
        "test_preds = scaled_pipe.predict(X_test)\n",
        "\n",
        "train_mse = mean_squared_error(y_train, train_preds)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "train_mae = mean_absolute_error(y_train, train_preds)\n",
        "train_r2 = r2_score(y_train, train_preds)\n",
        "\n",
        "test_mse = mean_squared_error(y_test, test_preds)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_mae = mean_absolute_error(y_test, test_preds)\n",
        "test_r2 = r2_score(y_test, test_preds)\n",
        "\n",
        "print('Training statistics')\n",
        "print(f'RMSE: {train_rmse}')\n",
        "print(f'r2: {train_r2}')\n",
        "print(f'MSE: {train_mse}')\n",
        "print(f'MAE: {train_mae}\\n')\n",
        "\n",
        "print('Test statistics')\n",
        "print(f'RMSE: {test_rmse}')\n",
        "print(f'r2: {test_r2}')\n",
        "print(f'MSE: {test_mse}')\n",
        "print(f'MAE: {test_mae}')\n",
        "# The RMSE is over $9000 with just the most relevant featues. This is a large error.\n",
        "# The error is reduced to over $8000 if all the data are used.\n",
        "# Running data by the state and region should help\n",
        "# Using the models of the cars should help also. This feature may need further cleaning.\n",
        "# One-hot encoding car models increases the features too much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thCRxLd6BZgJ"
      },
      "outputs": [],
      "source": [
        "# Adjust the weights for each feature\n",
        "coef_list = []\n",
        "param_grid = {'alpha': [0.001, 1.0, 10.0, 100.0]}\n",
        "ridge = Ridge()\n",
        "ridge.fit(X_train, y_train)\n",
        "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='neg_root_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Best alpha found:\", grid_search.best_params_['alpha'])\n",
        "print(\"Best negative root mean squared error:\", grid_search.best_score_)\n",
        "grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
        "print(\"Cross-validation scores:\\n\", grid_search_results[['param_alpha', 'split0_test_score', 'rank_test_score']])\n",
        "# The weights of the selected features are about equal.\n",
        "# This means the selected features are equally important to determining the price of cars."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
